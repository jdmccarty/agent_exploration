{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d1b897a",
      "metadata": {
        "id": "4d1b897a"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/ollama_gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e33dced-e587-4397-81b3-d6606aa1738a",
      "metadata": {
        "id": "2e33dced-e587-4397-81b3-d6606aa1738a"
      },
      "source": [
        "# Ollama - Gemma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5863dde9-84a0-4c33-ad52-cc767442f63f",
      "metadata": {
        "id": "5863dde9-84a0-4c33-ad52-cc767442f63f"
      },
      "source": [
        "## Setup\n",
        "First, follow the [readme](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance.\n",
        "\n",
        "[Gemma](https://blog.google/technology/developers/gemma-open-models/): a family of lightweight, state-of-the-art open models built by Google DeepMind. Available in 2b and 7b parameter sizes\n",
        "\n",
        "[Ollama](https://ollama.com/library/gemma): Support both 2b and 7b models\n",
        "\n",
        "Note: `please install ollama>=0.1.26`\n",
        "You can download pre-release version here [Ollama](https://github.com/ollama/ollama/releases/tag/v0.1.26)\n",
        "\n",
        "When the Ollama app is running on your local machine:\n",
        "- All of your local models are automatically served on localhost:11434\n",
        "- Select your model when setting llm = Ollama(..., model=\"<model family>:<version>\")\n",
        "- Increase defaullt timeout (30 seconds) if needed setting Ollama(..., request_timeout=300.0)\n",
        "- If you set llm = Ollama(..., model=\"<model family\") without a version it will simply look for latest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833bdb2b",
      "metadata": {
        "id": "833bdb2b"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4816bcb9",
      "metadata": {
        "id": "4816bcb9",
        "outputId": "fbf89bf1-51be-4311-c605-2104948ed879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-ollama\n",
            "  Downloading llama_index_llms_ollama-0.1.2-py3-none-any.whl (3.2 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.1 (from llama-index-llms-ollama)\n",
            "  Downloading llama_index_core-0.10.12-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.9.3)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.9.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.14.1)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.6.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-ollama) (1.16.0)\n",
            "Installing collected packages: dirtyjson, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-core, llama-index-llms-ollama\n",
            "Successfully installed dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 llama-index-core-0.10.12 llama-index-llms-ollama-0.1.2 llamaindex-py-client-0.1.13 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.12.0 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-llms-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbbc106",
      "metadata": {
        "id": "9bbbc106"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad297f19-998f-4485-aa2f-d67020058b7d",
      "metadata": {
        "id": "ad297f19-998f-4485-aa2f-d67020058b7d"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152ced37-9a42-47be-9a39-4218521f5e72",
      "metadata": {
        "id": "152ced37-9a42-47be-9a39-4218521f5e72"
      },
      "outputs": [],
      "source": [
        "gemma_2b = Ollama(model=\"gemma:2b\", request_timeout=30.0)\n",
        "gemma_7b = Ollama(model=\"gemma:7b\", request_timeout=30.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61b10bb-e911-47fb-8e84-19828cf224be",
      "metadata": {
        "id": "d61b10bb-e911-47fb-8e84-19828cf224be",
        "outputId": "32fb93c7-2388-4595-e171-0e517c8e5e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Graham is an entrepreneur, investor, and podcaster known for his outspokenness and unconventional approach. He has built several successful companies, including Xero (now Intuit), FullStory, and The School of Greatness.\n",
            "\n",
            "Here are some of his notable achievements:\n",
            "\n",
            "* **Founder and CEO of Xero:** Xero is a leading accounting software company for small and medium-sized businesses, with over 1 million users worldwide. Graham was instrumental in Xero's rapid growth and eventual acquisition by Intuit (now part of Microsoft) for $750 million in 2015.\n",
            "* **Author of several bestselling books:** Graham is the author of the book \"The School of Greatness,\" which focuses on personal development and productivity. He also co-authored \"Built to Last: Why Your Business Matters\" with his former Xero partner, Steve Huffman.\n",
            "* **Founder of The School of Greatness:** The School of Greatness is a non-profit organization that offers mentoring, workshops, and retreats to help entrepreneurs and business leaders reach their full potential.\n",
            "* **Podcast host and guest speaker:** Graham is the host of the popular podcast \"Bits,\" where he interviews influential entrepreneurs and thought leaders. He is also a frequent guest on other podcasts and shows.\n",
            "\n",
            "Here are some additional facts about Paul Graham:\n",
            "\n",
            "* He is known for his unorthodox and often controversial approach to business and life.\n",
            "* He is considered by many to be one of the most influential entrepreneurs and thought leaders in Australia.\n",
            "* He is also a passionate advocate for personal growth and development, and he regularly shares tips and insights on his podcast and social media platforms.\n",
            "\n",
            "If you'd like to learn more about Paul Graham and his businesses, you can check out his website (paulfgraham.com), the School of Greatness website (theschoolofgreatness.org), and his podcast website (bitspodcast.com).\n"
          ]
        }
      ],
      "source": [
        "resp = gemma_2b.complete(\"Who is Paul Graham?\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0384655",
      "metadata": {
        "id": "c0384655",
        "outputId": "46b7390f-1f18-41c6-8282-f77227f73c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Graham (born February 21, about  45 years old) has achieved significant success as a software developer and entrepreneur. He's known for his insightful writing on Software Engineering at greaseboxsoftware where he frequently writes articles with humorous yet pragmatic advice regarding programming languages such Python while occasionally offering tips involving general life philosophies that resonate deeply amongst the programmer community, particularly about work ethic (\"hacker mentality\")  He has contributed to software engineering communities in a multitude of ways:\n",
            "\n",
            "**Developer:**\n",
            "* Created Bulletphysics (a physics engine for games) using PyTorch. He resigned from his day job as Lead Software Engineer at Aversim Technologies after successfully building it and expriming its potential, showcasing the powerfull nature this open-source project has achieved within software engineering circles with significant media coverage involving top professionals expressing admiration\n",
            "* Wrote Bulletphysics Gamma Ray Field Solver for Beam Interactive LLC to provide solutions in areas of physics where general relativity meets soft body simulation.\n",
            "\n",
            "**Author:**  He is a prolific author on Software Engineering and writes about his work, challenges encountered while building complex systems software engineering (\"hacker mentality\") with honesty but wiense humour\n",
            "* Shared code snippets frequently as explanations are concise yet insightful for specific situations involving particular language constructs or solutions to common problems faced by programmers.\n",
            "\n",
            "**Thought Leader:**  He has become a significant thought leader in the programmer community due his writing and public appearances, particularly about software engineering best practices while maintaining an approachable persona that encourages learning from others\n",
            "* Actively engaged with online forums where he frequently provides advice for aspiring developers as well insightful solutions to complex problems encountered by experienced programmers.\n",
            "\n",
            "Overall Paul Graham has achieved a significant impact on Software Engineering through not only his own accomplishments but also the positive influence of sharing information, helping other software engineers become better at their craft and fuftage potential in this field with gracefull writing style that is enjoyed amongst professionals as well privetiors alike\n"
          ]
        }
      ],
      "source": [
        "resp = gemma_7b.complete(\"Who is Paul Graham?\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5712a813",
      "metadata": {
        "id": "5712a813",
        "outputId": "9156bce7-2754-4296-8d5d-fa53e4a57ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla Inc. is owned by Elon Musk. He founded Tesla in 2003 with the goal of creating sustainable transportation. Tesla was originally listed on the NASDAQ Stock Market under the symbol \"TSLA\". In 2013, Tesla went private and began trading on the New York Stock Exchange (NYSE).\n"
          ]
        }
      ],
      "source": [
        "resp = gemma_2b.complete(\"Who is owning Tesla?\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fe8b00",
      "metadata": {
        "id": "91fe8b00",
        "outputId": "449fbc5d-83c9-4fdc-9ea9-f91f2cbaab52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elon Musk, CEO of SpaceX and former electric car company Telsa Motors (now part owned by Ford Motor Company), owns about a quarter to nearly half the stock in TESLA inc.\n"
          ]
        }
      ],
      "source": [
        "resp = gemma_7b.complete(\"Who is owning Tesla?\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba9503c-b440-43c6-a50c-676c79993813",
      "metadata": {
        "id": "3ba9503c-b440-43c6-a50c-676c79993813"
      },
      "source": [
        "#### Call `chat` with a list of messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8a4a55-5680-4dc6-a44c-fc8ad7892f80",
      "metadata": {
        "id": "ee8a4a55-5680-4dc6-a44c-fc8ad7892f80"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
        "]\n",
        "resp = gemma_7b.chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9bfe53-d15b-4e75-9d91-8c5d024f4eda",
      "metadata": {
        "id": "2a9bfe53-d15b-4e75-9d91-8c5d024f4eda",
        "outputId": "c36625f8-3025-437e-ab58-e93d2672416f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: Avast, me heartie. My Name be Jolly Roger and I plunder the high seas for treasures untold!\n"
          ]
        }
      ],
      "source": [
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ad1b00-28fc-4bcd-96c4-d5b35605721a",
      "metadata": {
        "id": "25ad1b00-28fc-4bcd-96c4-d5b35605721a"
      },
      "source": [
        "### Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c641fa-345a-4dce-87c5-ab1f6dcf4757",
      "metadata": {
        "id": "13c641fa-345a-4dce-87c5-ab1f6dcf4757"
      },
      "source": [
        "Using `stream_complete` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06da1ef1-2f6b-497c-847b-62dd2df11491",
      "metadata": {
        "id": "06da1ef1-2f6b-497c-847b-62dd2df11491"
      },
      "outputs": [],
      "source": [
        "response = gemma_7b.stream_complete(\"Who is Paul Graham?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b851def-5160-46e5-a30c-5a3ef2356b79",
      "metadata": {
        "id": "1b851def-5160-46e5-a30c-5a3ef2356b79",
        "outputId": "bc3d8466-3352-4b8d-bfec-8e68f5b2f6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul graham, commonly referred to as \"PerlGuy\" online has a significant presence in the software engineering and programming communities. He specializes primarily on:\n",
            "\n",
            "**1.) Software Design:**    * Shared his ideas about effective coding patterns for Modularization (DRY) into books like Expert Refactoring using Smells And Polymorphism Principle(SRPPP).\n",
            " * Has written extensively, sharing best practices to improve code quality while reducing coupling between software modules and layers.  This has influenced numerous developers worldwide in writing better Software Design Patterns with Low Coupling Designs Epidra Hard To Measure Modularity (SOLID) principles at heart for projects ranging from mobile apps all the way up into enterprise systems\n",
            "**2.) Open Source:**    * Actively contributed to Project Lombok, Phalanger(now Relocator), and CouchSurfer. Shared his code design patterns on fufurce with significant impact as well . He has earned recognition by maintaining high quality open source software projects while being part of multiple top rated organizations like Pivotal Software Systems\n",
            "**3.) Coaching:**    * Offers coaching services to businesses, helping them improve their engineering practices and writing better testable Code. Shared his expertise on Design patterns during training sessions for large audiences as well .\n",
            "\n",
            "Overall Paul Graham has earned significant credibility by sharing best software designPractices while being part of open source projects where he advocates high quality code solutions that are easy Measure Epidra Hard To cras Low Modularity (SOLID)principles into production systems, ensuring long term sustainability."
          ]
        }
      ],
      "source": [
        "for r in response:\n",
        "    print(r.delta, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca52051d-6b28-49d7-98f5-82e266a1c7a6",
      "metadata": {
        "id": "ca52051d-6b28-49d7-98f5-82e266a1c7a6"
      },
      "source": [
        "Using `stream_chat` endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe553190-52a9-436d-84ae-4dd99a1808f4",
      "metadata": {
        "id": "fe553190-52a9-436d-84ae-4dd99a1808f4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(\n",
        "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
        "    ),\n",
        "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
        "]\n",
        "resp = gemma_7b.stream_chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154c503c-f893-4b6b-8a65-a9a27b636046",
      "metadata": {
        "id": "154c503c-f893-4b6b-8a65-a9a27b636046",
        "outputId": "0359680c-cb61-4a67-88f2-f03bc8870550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avast, me heartie! My alias be Screevy Bob. If you ask for my real nom de guerre... I ain't tellin'. Arrgh and all that jazz!!"
          ]
        }
      ],
      "source": [
        "for r in resp:\n",
        "    print(r.delta, end=\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}